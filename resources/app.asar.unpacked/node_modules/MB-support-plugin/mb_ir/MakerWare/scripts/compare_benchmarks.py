#!/usr/bin/python

import run_python_34
run_python_34.bootstrap_v34()

import os
import sys
import argparse
import json
import glob
import inspect
import datetime

import pygrue
import benchmark_tests

"""
Utility to nicely display the differences between two sets of benchmarks.
"""

def list_builds(args, build_results):

    print("Listing builds found in results:\n")

    nearest_keys = \
        benchmark_tests.GrueBenchmarkDiff.nearest_builds_to(
            datetime.datetime.now(), build_results, n=0)

    build_results = \
        benchmark_tests.GrueBenchmarkDiff.get_similar_builds(build_results);

    for build_key in nearest_keys:

        similar_results = build_results[build_key]

        if len(similar_results) > 1:
            print("WARNING: Multiple similar builds:")

        for _, results in similar_results.items():

            print((benchmark_tests.GrueBenchmarkDiff.build_str(results)))

        print("")

def build_matches(build_key, target_values):
    for i in range(0, len(target_values)):
        if not build_key[i][1].startswith(target_values[i]):
            return False
    return True

def select_builds_by_str(build_key_str, build_results):

    target_values = build_key_str.split(":")

    build_keys = []
    for build_key, _ in build_results.items():
        if build_matches(build_key, target_values):
            build_keys.append(build_key)

    return build_keys

DEFAULT_LOCAL_RESULT_DIR = \
    os.path.join(os.path.dirname(inspect.getfile(benchmark_tests)), 
                 'saved_benchmark_results')

DESCRIPTION = \
    """Compare benchmark results on the command line"""

if __name__ == "__main__":

    parser = argparse.ArgumentParser(
        description=DESCRIPTION)

    parser.add_argument(
        '--local', dest='input_local_benchmarks', action='store_true',
        help='Plot benchmark experiments saved locally.\n\n\n',
        default=False)  

    parser.add_argument(
        '--input-filename', dest='input_fileglobs', action='append',
        help='Input filename or glob file search.',
        default=[])

    parser.add_argument(
        '--timestamp', dest='timestamps', action='append',
        help='Select benchmark build by timestamp it was run on.'
             'Closest timestamp matches.',
        default=[])

    parser.add_argument(
        '--commit', dest='build_key_strs', action='append',
        help='Select benchmarks by commit.',
        default=[])


    parser.add_argument(
        '--most-recent', dest='most_recent', action='store_true',
        help='Select two most recent benchmarked builds.',
        default=False)

    args = parser.parse_args(sys.argv[1:])

    if args.input_local_benchmarks:
        args.input_fileglobs.append(
            os.path.join(DEFAULT_LOCAL_RESULT_DIR, '*'))

    input_results = []

    for input_fileglob in args.input_fileglobs:
        input_filenames = glob.glob(input_fileglob)
        for input_filename in input_filenames:
            with open(input_filename, 'r') as input_file:
                input_results.extend(json.load(input_file))

    build_results = \
        benchmark_tests.GrueBenchmarkDiff.get_builds(input_results);
    
    build_keys = []

    if args.most_recent and (args.timestamps or args.build_key_strs):
        raise Exception("Cannot combine --most-recent and timestamp/commit "
                        "options.")

    if args.timestamps:
        for timestamp in args.timestamps:

            dtime = pygrue.utils.parse_timestamp(timestamp)
            build_key = \
                benchmark_tests.GrueBenchmarkDiff.nearest_build_to(
                    dtime, build_results)

            build_keys.append(build_key)

    if args.build_key_strs:
        for build_key_str in args.build_key_strs:
            target_keys = select_builds_by_str(build_key_str, build_results)

            if len(target_keys) < 1:
                raise Exception("Build key %s does not match any builds in the "
                                "benchmark data." % build_key_str)
            if len(target_keys) > 1:
                raise Exception("Build key %s matches multiple builds in the "
                                "benchmark data:\n%s" % (build_key_str,
                                                         target_keys))
            build_keys.append(target_keys[0])

    if args.most_recent:
        build_keys = \
            benchmark_tests.GrueBenchmarkDiff.nearest_builds_to(
                datetime.datetime.now(), build_results, n=2)

        build_keys.reverse()

        if len(build_keys) < 2:
            raise Exception("Two recent builds not found to compare.")

    if not build_keys:
        list_builds(args, build_results);
        sys.exit(0)

    if len(build_keys) != 2:
        raise Exception("Need exactly two builds to compare, have %s builds." %
                        len(build_keys))

    diff = benchmark_tests.GrueBenchmarkDiff(build_keys[0], build_keys[1], 
                                             input_results)

    diff.print_summary();